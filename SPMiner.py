import os
import re
import numpy as np
from pdf2image import convert_from_path
from PIL import Image
from decimer_segmentation import (
    segment_chemical_structures,
    segment_chemical_structures,
)
import pathlib
from PyPDF2 import PdfWriter, PdfReader
import dashscope
from DECIMER import predict_SMILES
from rdkit import Chem
from rdkit.Chem import Draw
import requests  # ç”¨äºå‘é€HTTPè¯·æ±‚
import json  # ç”¨äºè§£æå’Œç”ŸæˆJSONæ ¼å¼æ•°æ®
import pdfplumber  # ç”¨äºè¯»å–PDFæ–‡ä»¶
import fitz  # PyMuPDF
import hashlib
import zipfile
import io
from collections import defaultdict
import csv
import time

allapi = {
    "mineru": "eyJ0eXBlIjoiSldUIiwiYWxnIjoiSFM1MTIifQ.eyJqdGkiOiI1MzMwMzgzOCIsInJvbCI6IlJPTEVfUkVHSVNURVIiLCJpc3MiOiJPcGVuWExhYiIsImlhdCI6MTc0MzA2NjgwOCwiY2xpZW50SWQiOiJsa3pkeDU3bnZ5MjJqa3BxOXgydyIsInBob25lIjoiMTk4NTU4MzI5MzIiLCJvcGVuSWQiOm51bGwsInV1aWQiOiIzZjRiZTU1Yy1hZTM4LTQ5MDQtODUxNy05MDY5ODA4M2ZhMGMiLCJlbWFpbCI6IiIsImV4cCI6MTc0NDI3NjQwOH0.fuP98caz394-3LXyAfGSqHc-vgNZTDBIUo91UxDlc8M4gtCwmCrBgFCmw_UAD_5xxlyYABaTXpbtzFTFKaTxLA",
    "qwen":"sk-4f7eae96162a4db99e74842c54ee7888",
    "deepseek":"sk-6946dfacf45f48f3a52427903404ff82"
    # å…¶ä»– API é…ç½®å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
}
file_note={
    "file_name":"test.pdf",
    "file_path":"/home/stu/yc/decimer/test.pdf"
}
os.path.splittext(fi)[-1]
MODELS = [
    {"name": "qwen-max", "type": "dashscope"},
    {"name": "deepseek-chat", "type": "deepseek"}
]  # å¯¹æ¯”æ¨¡å‹é…ç½®
params = ['åŒ–åˆç‰©åç§°','å¸æ”¶æ³¢é•¿(Î»abs)','å‘å°„æ³¢é•¿(Î»em)','åŠå³°å®½(FWHM)','s1æ€èƒ½çº§(Es1)','t1æ€èƒ½çº§(Et1)','å¸¦éš™(Egap)','HOMOèƒ½çº§','LUMOèƒ½çº§','PLQY(å…‰è‡´å‘å…‰é‡å­äº§é‡)','delta_Est(å•ä¸‰çº¿æ€èƒ½å·®)',"CIE(è‰²åæ ‡)","EQE(å¤–é‡å­æ•ˆç‡)","å™¨ä»¶ç»“æ„","knr(éè¾å°„é€Ÿç‡)"]
image_folder = "/home/stu/yc/decimer/pdf_images"#æå–å›¾ç‰‡ä¿å­˜åˆ°è¯¥ä½ç½®
raw_name="BN-TP"
poppler_path = "/home/software/anaconda3/envs/decimer/bin"


def pdf2md(file_note: dict, allapi: dict) -> string:
    """è¾“å…¥pdfï¼Œå¾—åˆ°mdæ–‡ä»¶"""
    mineru_api = allapi.get("mineru")
    file_name = file_note.get("file_name")
    url = 'https://mineru.net/api/v4/file-urls/batch'
    header = {
        'Content-Type': 'application/json',
        "Authorization": f"Bearer {mineru_api}"
    }
    data = {
        "enable_formula": True,
        "language": "en",
        "layout_model": "doclayout_yolo",
        "enable_table": True,
        "files": [
            {"name": f"{file_name}", "is_ocr": True, "data_id": "abcd"}
        ]
    }
    # ç¬¬ä¸€æ­¥ï¼šè·å–æ–‡ä»¶ä¸Šä¼ é“¾æ¥
    response = requests.post(url, headers=header, data=json.dumps(data))
    result = response.json()

    file_path = file_note["file_path"]
    file_urls = result["data"]["file_urls"]
    
    # ç¬¬äºŒæ­¥ï¼šä¸Šä¼  PDF æ–‡ä»¶
    with open(file_path, 'rb') as f:
        res_upload = requests.put(file_urls[0], data=f)
        if res_upload.status_code != 200:
            raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {res_upload.status_code}")

    batch_id = result["data"]["batch_id"]
    url = f'https://mineru.net/api/v4/extract-results/batch/{batch_id}'

    # è½®è¯¢æ£€æŸ¥å¤„ç†ç»“æœ
    start_time = time.time()
    timeout = 60  # è¶…æ—¶æ—¶é—´60ç§’
    while True:
        # è½®è¯¢æŸ¥è¯¢ API
        response = requests.get(url, headers=header)
        back_information = response.json()

        # è·å– full_zip_url çš„æ­£ç¡®æ–¹å¼
        extract_result = back_information.get('data', {}).get('extract_result', [])
        if extract_result:
            full_zip_url = extract_result[0].get('full_zip_url')
            
            if full_zip_url:
                zip_response = requests.get(full_zip_url)
                
                # è§£å‹ ZIP æ–‡ä»¶
                with zipfile.ZipFile(io.BytesIO(zip_response.content)) as zip_ref:
                    zip_ref.extractall("extracted_files")  # è§£å‹åˆ°æœ¬åœ°ç›®å½•
                
                # æŸ¥æ‰¾å¹¶è¯»å– Markdown æ–‡ä»¶
                md_content = ""
                for file_name in os.listdir("extracted_files"):
                    if file_name.endswith(".md"):
                        with open(os.path.join("extracted_files", file_name), "r", encoding="utf-8") as md_file:
                            md_content = md_file.read()
                        break  # è¯»å–ç¬¬ä¸€ä¸ª Markdown æ–‡ä»¶åé€€å‡º

                if md_content:
                    print("Markdown æ–‡ä»¶å·²æ­£ç¡®å‚¨å­˜åœ¨å˜é‡ md_content ä¸­")
                    return md_content
                else:
                    print("æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
                    return None

        # æ£€æŸ¥è¶…æ—¶
        if time.time() - start_time > timeout:
            print("è¶…æ—¶ï¼Œæœªè·å¾— full_zip_url")
            return back_information  # è¿”å›æœ€åä¸€æ¬¡çš„ç»“æœ

        # ç­‰å¾…3ç§’åç»§ç»­æ£€æµ‹
        time.sleep(3)

# è°ƒç”¨å‡½æ•°
md_content = pdf2md(file_note, allapi)

class MD2CSV:
    def __init__(self, md_content, MODELS, params, allapi):
        self.md_content = md_content
        self.MODELS = MODELS
        self.params = params
        self.allapi = allapi
        self.headers = []
        self.all_models_data = []
        self.global_conditions = defaultdict(set)
        
        # åˆå§‹åŒ–å¤„ç†ç®¡é“
        self.promote_gen = {
            "qwen": self.__gen_promote4qwen,
            "deepseek": self.__gen_promote4deepseek
        }
        self.model_run = {
            "qwen": self.__run_qwen,
            "deepseek": self.__run_deepseek
        }
        self.string2csv = {
            "qwen": self.__parse_qwen,
            "gpt": self.__parse_deepseek
        }
        
    def __gen_promote4qwen(self):
        return (
                 f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–åŒ–åˆç‰©çš„åç§°ã€æ€§è´¨ã€æµ‹è¯•æ¡ä»¶ã€æµ‹è¯•ç»“æœåŠå…¶å•ä½ï¼ŒæŒ‰ç…§ä¸‹é¢çš„æ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ å…¶ä»–é¢å¤–çš„æ ‡è®°ï¼‰ã€‚å¦‚æœæŸä¸ªå‚æ•°ã€å•ä½æˆ–æµ‹è¯•æ¡ä»¶æœªèƒ½æ£€ç´¢åˆ°ï¼Œè¯·æ ‡è®°ä¸ºNAã€‚å¦‚æœæœ‰å…¶ä»–é¢å¤–çš„å‚æ•°ï¼Œé™„åœ¨åè¾¹ï¼Œä¸”ä¿æŒä¸å‰é¢å‚æ•°ä¸€è‡´çš„æ ¼å¼ã€‚\n" +
                 f"è¾“å‡ºæ ¼å¼ï¼š\n" +
                 f"æµ‹è¯•æ¡ä»¶: æµ‹è¯•ç»“æœ: å•ä½: \n" +
                 f"è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼é€ä¸€è¾“å‡ºæ¯ä¸ªæ£€ç´¢åˆ°çš„å‚æ•°ã€‚ä¾‹å¦‚: \n" +
                 f"å‘å°„æ³¢é•¿(Î»em): (1Ã—10^(-5)M, 298K, toluene): 523: nm \n" +
                 f"å‘å°„æ³¢é•¿(Î»em): PhCzBCz doped film (3wt% doping concentration): 528: nm\n" +
                 f"å‚æ•°åŒ…æ‹¬:{', '.join(params)}\næ–‡æœ¬ï¼š{md_content}"
        )

    def __gen_promote4deepseek(self):
        return (
                 f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–åŒ–åˆç‰©çš„åç§°ã€æ€§è´¨ã€æµ‹è¯•æ¡ä»¶ã€æµ‹è¯•ç»“æœåŠå…¶å•ä½ï¼ŒæŒ‰ç…§ä¸‹é¢çš„æ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ å…¶ä»–é¢å¤–çš„æ ‡è®°ï¼‰ã€‚å¦‚æœæŸä¸ªå‚æ•°ã€å•ä½æˆ–æµ‹è¯•æ¡ä»¶æœªèƒ½æ£€ç´¢åˆ°ï¼Œè¯·æ ‡è®°ä¸ºNAã€‚å¦‚æœæœ‰å…¶ä»–é¢å¤–çš„å‚æ•°ï¼Œé™„åœ¨åè¾¹ï¼Œä¸”ä¿æŒä¸å‰é¢å‚æ•°ä¸€è‡´çš„æ ¼å¼ã€‚\n" +
                 f"è¾“å‡ºæ ¼å¼ï¼š\n" +
                 f"æµ‹è¯•æ¡ä»¶: æµ‹è¯•ç»“æœ: å•ä½: \n" +
                 f"è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼é€ä¸€è¾“å‡ºæ¯ä¸ªæ£€ç´¢åˆ°çš„å‚æ•°ã€‚ä¾‹å¦‚: \n" +
                 f"å‘å°„æ³¢é•¿(Î»em): (1Ã—10^(-5)M, 298K, toluene): 523: nm \n" +
                 f"å‘å°„æ³¢é•¿(Î»em): PhCzBCz doped film (3wt% doping concentration): 528: nm\n" +
                 f"å‚æ•°åŒ…æ‹¬:{', '.join(params)}\næ–‡æœ¬ï¼š{md_content}"
        )
   
    def __run_qwen(self, model_name):
        try:
            response = dashscope.Generation.call(
                api_key=self.allapi.get("qwen"),
                model=model_name,
                messages=[{'role': 'user', 'content': self.promote_gen["qwen"]()}],
                result_format='text'
            )
            return response.output.text.strip() if response and response.output else ""
        except Exception as e:
            print(f"Qwenæ¥å£é”™è¯¯: {str(e)}")
            return ""

    def __run_deepseek(self, model_name):
        try:
            # å®šä¹‰ API è¯·æ±‚å‚æ•°
            deepseek_api=allapi.get("deepseek")
            url = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {deepseek_api}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "deepseek-chat",
                "messages":[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ–å­¦ä¿¡æ¯æå–åŠ©æ‰‹ï¼Œè¯·æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼å›ç­”é—®é¢˜"},
                    {"role": "user", "content": self.promote_gen["deepseek"]()}
                ],
            "stream": False
            }
            # å‘é€è¯·æ±‚
            response = requests.post(url, headers=headers, data=json.dumps(data))
            response.raise_for_status()  # è‡ªåŠ¨å¤„ç† HTTP é”™è¯¯ç 
    
            # è§£æå“åº”
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"deepseekæ¥å£é”™è¯¯: {str(e)}")
            return ""

    def __parse_qwen(self, response_text, model_name):
        model_params = defaultdict(dict)
        for line in response_text.split('\n'):
            line = line.strip()
            if not line or ':' not in line:
                continue
                
            # ç»Ÿä¸€å¤„ç†ä¸­è‹±æ–‡å†’å·
            parts = re.split(r'[:ï¼š]', line, 3)
            if len(parts) >= 4:
                param, cond, val, unit = (p.strip() for p in parts)
                final_val = f"{val} {unit}" if unit not in ("NA", "") else val
                model_params[param][cond] = final_val
                self.global_conditions[param].add(cond)
        return model_params

    def __parse_deepseek(self, response_text, model_name):
        model_params = defaultdict(dict)
        for line in response_text.split('\n'):
            line = line.strip()
            if not line or ':' not in line:
                continue
                
            # é€‚é…GPTå¯èƒ½ä½¿ç”¨çš„ä¸åŒåˆ†éš”ç¬¦
            parts = re.split(r'\s*:\s*', line, 3)
            if len(parts) >= 4:
                param, cond, val, unit = (p.strip() for p in parts)
                final_val = f"{val} {unit}" if unit.lower() not in ("na", "n/a") else val
                model_params[param][cond] = final_val
                self.global_conditions[param].add(cond)
        return model_params

    def __generate_headers(self):
        headers = ["Model"]
        for param in self.params:
            if param in self.global_conditions:
                sorted_conditions = sorted(
                    self.global_conditions[param],
                    key=lambda x: next((
                        idx for model_data in self.all_models_data
                        for idx, cond in enumerate(model_data['params'].get(param, {})) if cond == x
                    ), float('inf'))
                )
                for cond in sorted_conditions:
                    headers.append(f"{param} [{cond}]")
        return headers

    def process_all_models(self):
        for model in self.MODELS:
            print(f"\næ­£åœ¨é€šè¿‡ [{model['name']}] æŸ¥è¯¢å‚æ•°...")
            model_type = model['type']

            response_text = self.model_run[model_type](model['name'])
            print(f"[{model['name']}] åŸå§‹å“åº”ï¼š\n{response_text or 'æ— å“åº”'}")

            # è§£æå“åº”
            if response_text and model_type in self.string2csv:
                parser = self.string2csv[model_type]
                model_params = parser(response_text, model['name'])
                
                self.all_models_data.append({
                    "name": model['name'],
                    "params": model_params
                })

        # ç”Ÿæˆæœ€ç»ˆè¡¨å¤´
        self.headers = self.__generate_headers()
        return self.headers, self.all_models_data, self.params, self.global_conditions



processor = MD2CSV(md_content, MODELS, params, allapi)
headers, all_models_data, params, global_conditions = processor.process_all_models()

md2csv = MD2CSV(md_content, MODELS, params, allapi)

# è°ƒç”¨å¤„ç†æ–¹æ³•ï¼Œè·å–Qwenå’ŒDeepseekçš„DataFrame
qwen_df, deepseek_df = md2csv.process_all_models()

# åˆå¹¶ DataFrameï¼ˆæŒ‰åŒ–åˆç‰©åç§°è¿›è¡Œåˆå¹¶ï¼Œä½¿ç”¨ outer åˆå¹¶æ–¹å¼ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«ä¿ç•™ï¼‰
merged_df = pd.merge(qwen_df, deepseek_df, on='åŒ–åˆç‰©åç§°', how='outer', suffixes=('_x', '_y'))
merged_df.drop('Model', axis=1, inplace=True)
# å»é™¤é‡å¤åˆ—ï¼Œå¹¶é€‰æ‹©ä¿ç•™ä¸€ä¸ªåˆ—
# è¿™é‡Œå¯ä»¥é€šè¿‡è‡ªå®šä¹‰é€»è¾‘ï¼Œé€‰æ‹©è¦ä¿ç•™çš„åˆ—ï¼Œå»é™¤ `_x` å’Œ `_y` åç¼€
for col in merged_df.columns:
    if '_x' in col:
        # è·å–å¯¹åº”çš„ _y åˆ—
        corresponding_col_y = col.replace('_x', '_y')
        if corresponding_col_y in merged_df.columns:
            # é€‰æ‹©ä¿ç•™ _x åˆ—ï¼Œç§»é™¤ _y åˆ—
            merged_df[col.replace('_x', '')] = merged_df[col]  # ä¿ç•™ _x åˆ—
            merged_df.drop([col, corresponding_col_y], axis=1, inplace=True)

# æ‰“å°åˆå¹¶åçš„ DataFrame
print(merged_df)

merged_path = "/home/stu/yc/decimer/csv_folder/merged_output.csv"


# æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
folder = os.path.dirname(merged_path)
if not os.path.exists(folder):
    os.makedirs(folder)

# ä¿å­˜æ–‡ä»¶

merged_df.to_csv(merged_path, index=False, encoding="utf-8")

merged_path = "/home/stu/yc/decimer/csv_folder/merged_output.csv"


# æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
folder = os.path.dirname(merged_path)
if not os.path.exists(folder):
    os.makedirs(folder)

# ä¿å­˜æ–‡ä»¶

merged_df.to_csv(merged_path, index=False, encoding="utf-8")

class PngFind:
    def __init__(self, allapi, raw_name, image_folder):
        self.allapi = allapi
        self.raw_name = raw_name
        self.image_folder = image_folder

    # --------------------------
    # æ–‡ä»¶å¤„ç†æ¨¡å—
    # --------------------------
    def natural_sort_key(self, s):
        """ç”Ÿæˆè‡ªç„¶æ’åºé”®"""
        return [
            int(text) if text.isdigit() else text.lower()
            for text in re.split(r'(\d+)', s)
        ]
    
    def extract_numbers(self, filename):
        """å¢å¼ºç‰ˆæ•°å­—æå–ï¼Œä¼˜å…ˆç»“æ„åŒ–åŒ¹é…"""
        # ä¼˜å…ˆåŒ¹é… page_X_img_Y æ ¼å¼
        structured_match = re.findall(r'(?:page|p)[_-]?(\d+).*?(?:img|image)[_-]?(\d+)', filename, re.IGNORECASE)
        if structured_match:
            return list(structured_match[0])
        
        # æ¬¡é€‰æ‰€æœ‰è¿ç»­æ•°å­—
        return re.findall(r'\d+', filename)
    
    def get_page_subpage(self, filename):
        """è·å–é¡µç å’Œå­é¡µå·ï¼ˆå­ç¼–å·ä»1å¼€å§‹ï¼‰"""
        numbers = self.extract_numbers(filename)
        
        # å¤„ç†é€»è¾‘
        if len(numbers) >= 2:
            return (int(numbers[0]), int(numbers[1]))
        elif numbers:
            return (int(numbers[0]), 1)  # å•ä¸ªæ•°å­—æ—¶å­ç¼–å·ä¸º1
        return (0, 1)  # æ— æ•°å­—æ–‡ä»¶é»˜è®¤0-1
    
    def process_image_files(self):
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶å¹¶ç”Ÿæˆæ˜ å°„å…³ç³»"""
        image_files = [f for f in os.listdir(self.image_folder) 
                      if f.lower().endswith(('.png', '.jpeg', '.jpg'))]
        
        # å››é‡æ’åºä¿éšœ
        image_files.sort(key=lambda x: (
            self.get_page_subpage(x)[0],   # ä¸»é¡µç 
            self.get_page_subpage(x)[1],   # å­é¡µç 
            self.natural_sort_key(x),      # è‡ªç„¶æ’åº
            x.lower()                      # å­—æ¯æ’åº
        ))
        
        # ç”Ÿæˆæ˜ å°„è¡¨
        number_mapping = []
        page_order = []
        for f in image_files:
            main, sub = self.get_page_subpage(f)
            number_mapping.append(f"{main}-{sub}")
            page_order.append(main)

        return image_files, number_mapping, page_order

    # --------------------------
    # APIå¤„ç†æ¨¡å—
    # --------------------------
    def call_dashscope_api(self, messages):
        """è°ƒç”¨å¤šæ¨¡æ€API"""
        response = dashscope.MultiModalConversation.call(
            api_key=self.allapi.get("qwen"),
            model="qwen-vl-max-latest",
            messages=messages
        )
        return response.output.choices[0].message.content[0]["text"]
    
    def process_api_response(self, out, max_index):
        """è§£æAPIå“åº”å†…å®¹"""
        # åˆ†æ®µå¤„ç†
        sections = [s.strip() for s in out.split('\n') if s.strip()]
        index_section = sections[0] if sections else ""
        name_sections = sections[1:] if len(sections) > 1 else []
        
        # è§£æç´¢å¼•
        indexes = []
        if index_section:
            indexes = [int(i) for i in index_section.split(',') if i.strip().isdigit()]
        
        # è§£æåç§°
        names = []
        for sec in name_sections:
            names.extend([n.strip() for n in sec.split(',')])
        
        # å¤„ç†ç´¢å¼•æœ‰æ•ˆæ€§
        valid_flags = []
        converted_indexes = []
        for idx in indexes:
            is_valid = 1 <= idx <= max_index
            valid_flags.append(is_valid)
            converted_indexes.append(idx-1 if is_valid else None)
        
        # åˆ†å­åç§°å¤„ç†
        missing_count = len([n for n in names if "æœªæ ‡æ˜" in n])
        missing_names = [f"æœªæ ‡æ˜-{i+1:02d}" for i in range(missing_count)]
        final_names = missing_names + [n for n in names if "æœªæ ‡æ˜" not in n]
    
        # æŸ¥æ‰¾BN-TP
        try:
            locations = final_names.index(self.raw_name)
        except ValueError:
            locations = None
        
        return {
            "raw_indexes": indexes,
            "valid_flags": valid_flags,
            "converted_indexes": converted_indexes,
            "number_mapping": final_names,
            "locations": locations
        }

    # --------------------------
    # ä¸»ç¨‹åº
    # --------------------------
    def final_output(self):
        # å¤„ç†å›¾ç‰‡æ–‡ä»¶
        sorted_files, number_codes, page_nums = self.process_image_files()
        max_index = len(sorted_files)  # æœ€å¤§æœ‰æ•ˆ1-basedç´¢å¼•
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": [{"text": "ä½ æ˜¯ä¸€ååŒ–å­¦ä¸“å®¶ï¼Œç›´æ¥ä»¥æ•°å­—å½¢å¼å‘Šè¯‰æˆ‘ç¬¬å‡ å¼ å›¾ç‰‡æœ‰åŒ–å­¦ç»“æ„ï¼Ÿ"}]
            },
            {
                "role": "user",
                "content": [{"image": os.path.join(self.image_folder, f)} for f in sorted_files] + 
                           [{"text": "æœ‰åŒ–å­¦ç»“æ„çš„å›¾ä¸­æœ‰å‡ ä¸ªåˆ†å­ç»“æ„ï¼Œè¯·æŒ‰ç…§ä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹çš„é¡ºåºå°†å›¾ç‰‡ä¸­çš„åŒ–å­¦åˆ†å­å¼åç§°åˆ—å‡ºï¼Œæ²¡æœ‰æ ‡æ³¨åç§°çš„åˆ†å­ç”¨æœªæ ‡æ˜-01ã€æœªæ ‡æ˜-02ç­‰ä»£æ›¿ï¼ŒæŠŠä»£æ›¿åå’ŒåŸæœ¬å­˜åœ¨çš„åˆ†å­åç§°è¾“å‡ºæˆé€—å·åˆ†éš”"}]
            }
        ]
        
        # è°ƒç”¨API
        api_response = self.call_dashscope_api(messages)
        
        # è§£æå“åº”
        result = self.process_api_response(api_response, max_index)

        # ç”Ÿæˆæœ€ç»ˆè¾“å‡º
        final_output = {
            "æ‰€æœ‰åŸå§‹ç´¢å¼•": result["raw_indexes"],
            "æœ‰æ•ˆç´¢å¼•": [i + 1 for i in result["converted_indexes"] if i is not None],
            "æ— æ•ˆç´¢å¼•": [result["raw_indexes"][i] for i, flag in enumerate(result["valid_flags"]) if not flag],
            "å¯¹åº”ç¼–å·": [number_codes[i] for i in result["converted_indexes"] if i is not None],
            "å¯¹åº”é¡µç ": [page_nums[i] for i in result["converted_indexes"] if i is not None],
            "åˆ†å­åˆ—è¡¨": result["number_mapping"],
            "åˆ†å­ä½ç½®": result["locations"]
        }

        # å°†æ‰€æœ‰ç»“æœå†™å…¥æ–‡æœ¬æ–‡ä»¶
        output_file = "output_results.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_output, f, ensure_ascii=False, indent=4)
        
        # æ‰“å°ç»“æœ
        print("APIåŸå§‹å“åº”:", api_response)
        print("æ–‡ä»¶æ€»æ•°:", len(sorted_files))
        print("æ‰€æœ‰åŸå§‹ç´¢å¼•:", final_output["æ‰€æœ‰åŸå§‹ç´¢å¼•"])
        print("æœ‰æ•ˆå›¾ç‰‡ç´¢å¼•:", final_output["æœ‰æ•ˆç´¢å¼•"])
        print("æ— æ•ˆå›¾ç‰‡ç´¢å¼•:", final_output["æ— æ•ˆç´¢å¼•"])
        print("å¯¹åº”å›¾ç‰‡ç¼–å·:", final_output["å¯¹åº”ç¼–å·"])
        print("å¯¹åº”ä¸»é¡µç :", final_output["å¯¹åº”é¡µç "])
        print("åˆ†å­åç§°åˆ—è¡¨:", final_output["åˆ†å­åˆ—è¡¨"])
        print("åˆ†å­ä½ç½®:", final_output["åˆ†å­ä½ç½®"])
        
        # åŠ¨æ€åˆ›å»ºå˜é‡å¹¶èµ‹å€¼
        page_numbers = final_output["å¯¹åº”é¡µç "]
        smiles_numbers = final_output["åˆ†å­ä½ç½®"]
        
        if not isinstance(smiles_numbers, list):
            smiles_numbers = [smiles_numbers]  # å¦‚æœæ˜¯å•ä¸ªæ•´æ•°ï¼Œå°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
        
        # åˆ›å»ºå…¨å±€å˜é‡
        for idx, page_num in enumerate(page_numbers):
            variable_name = f"chemical{idx + 1:02d}"
            globals()[variable_name] = page_num  
        
        for idx, smiles_num in enumerate(smiles_numbers):
            variable_name = f"smiles{idx + 1:02d}"
            globals()[variable_name] = smiles_num
        
        # æ‰“å°ç»“æœæ¥éªŒè¯
        print("æ‰€æœ‰åŒ–å­¦å“å˜é‡:")
        for idx, page_num in enumerate(page_numbers):
            variable_name = f"chemical{idx + 1:02d}"
            print(f"{variable_name}: {globals()[variable_name]}")
        
        print("æ‰€æœ‰åŒ–å­¦åç§°å˜é‡:")
        for idx, smiles_num in enumerate(smiles_numbers):
            variable_name = f"smiles{idx + 1:02d}"
            print(f"{variable_name}: {globals()[variable_name]}")
        
        print(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

png_find=PngFind(allapi,raw_name,image_folder)
final_output=png_find.final_output()

def extract_pdf_and_decode_smiles(file_note, segments, poppler_path=poppler_path, together=False):
    """
    ä»PDFä¸­æå–é¡µé¢å¹¶è§£ç åŒ–å­¦ç»“æ„ä¸ºSMILESç¬¦å·ã€‚

    file_note: dict, åŒ…å«PDFè·¯å¾„çš„å­—å…¸
    segments: [(start, end)], æ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„ï¼Œè¡¨ç¤ºéœ€è¦æå–çš„é¡µç èŒƒå›´
    poppler_path: str, Popplerè·¯å¾„ï¼Œç”¨äºå°†PDFé¡µé¢è½¬æ¢ä¸ºå›¾åƒï¼ˆéœ€è¦Popplerè¿›è¡ŒPDFè½¬å›¾åƒï¼‰
    together: bool, æ˜¯å¦å°†æ‰€æœ‰é¡µé¢åˆå¹¶æˆä¸€ä¸ªPDFï¼ˆé»˜è®¤ä¸ºFalseï¼‰
    """
    # æå–PDFé¡µé¢
    file_path = file_note.get("file_path")
    pdf_writer = PdfWriter()
    pdf_writer_segment = PdfWriter()
    p = Path(file_path)

    with open(file_path, 'rb') as read_stream:
        pdf_reader = PdfReader(read_stream)

        for segment in segments:
            start_page, end_page = segment  # ç›´æ¥è§£åŒ…å…ƒç»„

            for page_num in range(start_page, end_page):
                if together:
                    pdf_writer.add_page(pdf_reader.pages[page_num])
                else:
                    pdf_writer_segment.add_page(pdf_reader.pages[page_num])

        # è¾“å‡ºæå–çš„PDFé¡µé¢
        if together:
            output = p.parent / p.with_stem(f'{p.stem}_extracted')
            with open(output, 'wb') as out:
                pdf_writer.write(out)
        else:
            for segment in segments:
                start_page, end_page = segment  # ç›´æ¥è§£åŒ…å…ƒç»„
                output = p.parent / p.with_stem(f'{p.stem}_pages_{start_page}-{end_page}')
                with open(output, 'wb') as out:
                    pdf_writer_segment.write(out)

    # ä½¿ç”¨Popplerå°†æå–çš„é¡µé¢è½¬æ¢ä¸ºå›¾åƒ
    if poppler_path:
        pages = convert_from_path(str(file_path), 300, poppler_path=poppler_path)
        print(f"æ€»å…±æå–äº† {len(pages)} å¼ å›¾åƒ")

        # ä»æå–çš„å›¾åƒä¸­é‡æ–°ç»„æˆæ–°çš„segments
        image_segments = segment_chemical_structures(np.array(pages[1]), expand=True, visualization=True)

        # ä»è¯†åˆ«åˆ°çš„å›¾åƒä¸­è·å–ç¬¬xä¸ªåˆ†å­
        img = Image.fromarray(image_segments[smiles01-1])
        img.save("test.png")
        img.show()

        # è§£ç SMILESç¬¦å·
        SMILES = predict_SMILES("test.png")
        print(f"ğŸ‰ è§£ç å¾—åˆ°çš„SMILES: {SMILES}")

        # å°†SMILESè½¬æ¢ä¸ºRDKitåˆ†å­å¹¶æ˜¾ç¤ºç»“æ„
        mol = Chem.MolFromSmiles(SMILES)
        mol_img = Draw.MolToImage(mol)
        mol_img.show()
    return SMILES
# ç¤ºä¾‹è°ƒç”¨å‡½æ•°
SMILES=extract_pdf_and_decode_smiles(file_note, [(chemical01,chemical02)], poppler_path=poppler_path, together=False)


